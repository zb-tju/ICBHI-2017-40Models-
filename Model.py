import pickle
import numpy as np
from keras.models import Sequential, Model
from keras.layers import (Conv1D, Dense, Flatten, Dropout, BatchNormalization, 
                         MaxPooling1D, GlobalAveragePooling1D, LSTM, GRU, 
                         Bidirectional, Input, Add, Concatenate, Activation,
                         AveragePooling1D, SeparableConv1D, DepthwiseConv1D,
                         MultiHeadAttention, LayerNormalization, Embedding,
                         Lambda, Reshape, RepeatVector, TimeDistributed,
                         LocallyConnected1D, GlobalMaxPooling1D, Layer)
from keras.optimizers import Adam, SGD, RMSprop
from keras.regularizers import l1, l2, l1_l2
from keras.callbacks import Callback
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
import keras.backend as K
from tqdm import tqdm
import time
import tensorflow as tf
import os
import random

# =================== è®¾ç½®éšæœºç§å­ ===================
def set_seeds(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§"""
    np.random.seed(seed)
    random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    # å¯é€‰ï¼šä¸ºäº†å®Œå…¨çš„ç¡®å®šæ€§ï¼Œä½†ä¼šé™ä½æ€§èƒ½
    # os.environ['TF_DETERMINISTIC_OPS'] = '1'

# =================== è®¾ç½®GPU ===================
def setup_gpu(gpu_id=0):
    """è®¾ç½®ä½¿ç”¨å“ªå—GPU"""
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # è®¾ç½®GPUå†…å­˜å¢é•¿
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            
            # å¦‚æœæŒ‡å®šäº†ç‰¹å®šGPU
            if gpu_id is not None and gpu_id < len(gpus):
                tf.config.experimental.set_visible_devices(gpus[gpu_id], 'GPU')
                print(f"ä½¿ç”¨GPU {gpu_id}: {gpus[gpu_id]}")
            else:
                print(f"ä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU: {gpus}")
        except RuntimeError as e:
            print(f"GPUè®¾ç½®é”™è¯¯: {e}")
    else:
        print("æ²¡æœ‰æ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPU")

# è®¾ç½®éšæœºç§å­
set_seeds(42)

# è®¾ç½®ä½¿ç”¨GPU 0ï¼ˆå¯ä»¥ä¿®æ”¹ä¸º0,1,2,3ä¸­çš„ä»»æ„ä¸€ä¸ªï¼‰
setup_gpu(gpu_id=2)


class MambaBlock(Layer):
    """Mambaå—çš„è‡ªå®šä¹‰å®ç°"""
    def __init__(self, d_model=128, **kwargs):
        super(MambaBlock, self).__init__(**kwargs)
        self.d_model = d_model
        
    def build(self, input_shape):
        super(MambaBlock, self).build(input_shape)
        self.dense_proj = Dense(self.d_model * 2)
        self.dense_delta = Dense(self.d_model, activation='softplus')
        self.dense_A = Dense(self.d_model, activation='sigmoid')
        self.dense_B = Dense(self.d_model)
        self.dense_C = Dense(self.d_model)
        
    def call(self, inputs):
        x_proj = self.dense_proj(inputs)
        x = x_proj[..., :self.d_model]
        z = x_proj[..., self.d_model:]
        
        delta = self.dense_delta(x)
        A = self.dense_A(x)
        B = self.dense_B(x)
        C = self.dense_C(x)
        
        # ç®€åŒ–çš„SSMè®¡ç®—
        h = x * A + B
        y = C * h
        
        # é—¨æ§
        y = y * K.sigmoid(z)
        return y

class LinearAttentionLayer(Layer):
    """çº¿æ€§æ³¨æ„åŠ›å±‚çš„è‡ªå®šä¹‰å®ç°"""
    def __init__(self, units=128, **kwargs):
        super(LinearAttentionLayer, self).__init__(**kwargs)
        self.units = units
        
    def build(self, input_shape):
        super(LinearAttentionLayer, self).build(input_shape)
        self.query_dense = Dense(self.units)
        self.key_dense = Dense(self.units)
        self.value_dense = Dense(self.units)
        
    def call(self, inputs):
        q = self.query_dense(inputs)
        k = self.key_dense(inputs)
        v = self.value_dense(inputs)
        
        # çº¿æ€§åŒ–
        q = K.exp(q)
        k = K.exp(k)
        
        # è®¡ç®—æ³¨æ„åŠ›
        kv = K.batch_dot(k, v, axes=[1, 1])
        qkv = K.batch_dot(q, kv, axes=[2, 1])
        
        return qkv

class PositionalEncoding(Layer):
    """ä½ç½®ç¼–ç å±‚"""
    def __init__(self, num_patches, d_model, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.num_patches = num_patches
        self.d_model = d_model
        
    def build(self, input_shape):
        super(PositionalEncoding, self).build(input_shape)
        # åˆ›å»ºå¯å­¦ä¹ çš„ä½ç½®ç¼–ç 
        self.pos_embed = self.add_weight(
            name='pos_embed',
            shape=(1, self.num_patches, self.d_model),
            initializer='zeros',
            trainable=True
        )
        
    def call(self, inputs):
        return inputs + self.pos_embed
# =================== æ•°æ®åŠ è½½å’Œé¢„å¤„ç† ===================
file_path = './Data/data.pkl'
with open(file_path, 'rb') as f:
    data = pickle.load(f)
    
X = np.array(list(zip(*data))[0])
y = np.array(list(zip(*data))[1])

sampling_strategy_over = {'Pneumonia': 150, 'Healthy': 150, 'URTI': 100, 'Bronchiectasis': 100, 'Bronchiolitis': 100}
smote = SMOTE(sampling_strategy=sampling_strategy_over, k_neighbors=5, random_state=42)
X_resampled_over, y_resampled_over = smote.fit_resample(X, y)

copd_indices = np.where(y_resampled_over == 'COPD')[0]
np.random.seed(42)
np.random.shuffle(copd_indices)
remove_indices = copd_indices[:len(copd_indices) // 2]

X_resampled = np.delete(X_resampled_over, remove_indices, axis=0)
y_resampled = np.delete(y_resampled_over, remove_indices)

print('Shape of X is', X_resampled.shape)
print('Shape of y is', y_resampled.shape)

le = LabelEncoder()
y_encoded = le.fit_transform(y_resampled)
y_resampled = to_categorical(y_encoded)

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)
X_train_reshaped = np.reshape(X_train, (X_train.shape[0], 189, 1))
X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 189, 1))

input_shape = (189, 1)


class Model1_BasicCNN:
    """åŸºç¡€CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=self.input_shape),
            Dropout(0.5),
            Flatten(),
            Dense(6, activation='softmax')
        ])
        return model

class Model2_DeepCNN:
    """æ·±å±‚CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(32, 3, activation='relu', input_shape=self.input_shape),
            Conv1D(64, 3, activation='relu'),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2),
            Dropout(0.3),
            Conv1D(256, 3, activation='relu'),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model3_BatchNormCNN:
    """æ‰¹æ ‡å‡†åŒ–CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 5, activation='relu', input_shape=self.input_shape),
            BatchNormalization(),
            Conv1D(128, 3, activation='relu'),
            BatchNormalization(),
            MaxPooling1D(2),
            Conv1D(256, 3, activation='relu'),
            BatchNormalization(),
            GlobalAveragePooling1D(),
            Dense(256, activation='relu'),
            BatchNormalization(),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model4_LSTM:
    """LSTMæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=self.input_shape),
            Dropout(0.3),
            LSTM(64, return_sequences=False),
            Dropout(0.3),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model5_BiLSTM:
    """åŒå‘LSTMæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Bidirectional(LSTM(64, return_sequences=True), input_shape=self.input_shape),
            Dropout(0.3),
            Bidirectional(LSTM(32)),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model6_GRU:
    """GRUæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            GRU(128, return_sequences=True, input_shape=self.input_shape),
            Dropout(0.3),
            GRU(64),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model7_CNN_LSTM:
    """CNN+LSTMæ··åˆæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 3, activation='relu', input_shape=self.input_shape),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2),
            LSTM(64, return_sequences=True),
            LSTM(32),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model8_CNN_GRU:
    """CNN+GRUæ··åˆæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 5, activation='relu', input_shape=self.input_shape),
            BatchNormalization(),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2),
            GRU(64, return_sequences=True),
            GRU(32),
            Dense(64, activation='relu'),
            Dropout(0.4),
            Dense(6, activation='softmax')
        ])
        return model

class Model9_ResidualCNN:
    """æ®‹å·®CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # ç¬¬ä¸€å±‚
        x = Conv1D(64, 3, padding='same', activation='relu')(inputs)
        
        # æ®‹å·®å—
        shortcut = Conv1D(64, 1, padding='same')(x)
        x = Conv1D(64, 3, padding='same', activation='relu')(x)
        x = Conv1D(64, 3, padding='same')(x)
        x = Add()([x, shortcut])
        x = Activation('relu')(x)
        
        x = MaxPooling1D(2)(x)
        x = Conv1D(128, 3, activation='relu')(x)
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model10_MultiScale:
    """å¤šå°ºåº¦CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # å¤šå°ºåº¦å·ç§¯
        conv1 = Conv1D(32, 3, activation='relu', padding='same')(inputs)
        conv2 = Conv1D(32, 5, activation='relu', padding='same')(inputs)
        conv3 = Conv1D(32, 7, activation='relu', padding='same')(inputs)
        
        # åˆå¹¶
        merged = Concatenate()([conv1, conv2, conv3])
        x = BatchNormalization()(merged)
        x = MaxPooling1D(2)(x)
        x = Conv1D(128, 3, activation='relu')(x)
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model11_DenseCNN:
    """å¯†é›†è¿æ¥CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 3, activation='relu', input_shape=self.input_shape),
            Conv1D(128, 3, activation='relu'),
            Conv1D(256, 3, activation='relu'),
            MaxPooling1D(2),
            Flatten(),
            Dense(512, activation='relu'),
            Dropout(0.5),
            Dense(256, activation='relu'),
            Dropout(0.3),
            Dense(128, activation='relu'),
            Dropout(0.2),
            Dense(6, activation='softmax')
        ])
        return model

class Model12_SeparableCNN:
    """å¯åˆ†ç¦»å·ç§¯æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            SeparableConv1D(64, 3, activation='relu', input_shape=self.input_shape),
            BatchNormalization(),
            SeparableConv1D(128, 3, activation='relu'),
            BatchNormalization(),
            MaxPooling1D(2),
            SeparableConv1D(256, 3, activation='relu'),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model13_AttentionLSTM:
    """æ³¨æ„åŠ›LSTMæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # LSTMå±‚
        lstm_out = LSTM(128, return_sequences=True)(inputs)
        
        # ç®€å•æ³¨æ„åŠ›æœºåˆ¶
        attention = Dense(1, activation='tanh')(lstm_out)
        attention = Flatten()(attention)
        attention = Activation('softmax')(attention)
        attention = RepeatVector(128)(attention)
        attention = Lambda(lambda x: K.permute_dimensions(x, (0, 2, 1)))(attention)
        
        # åº”ç”¨æ³¨æ„åŠ›
        merged = Lambda(lambda x: x[0] * x[1])([lstm_out, attention])
        merged = Lambda(lambda x: K.sum(x, axis=1))(merged)
        
        output = Dense(128, activation='relu')(merged)
        output = Dropout(0.5)(output)
        output = Dense(6, activation='softmax')(output)
        
        model = Model(inputs, output)
        return model

class Model14_WideDeepCNN:
    """å®½æ·±CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # å®½éƒ¨åˆ†
        wide = Conv1D(256, 1, activation='relu')(inputs)
        wide = GlobalAveragePooling1D()(wide)
        
        # æ·±éƒ¨åˆ†
        deep = Conv1D(64, 3, activation='relu')(inputs)
        deep = Conv1D(128, 3, activation='relu')(deep)
        deep = MaxPooling1D(2)(deep)
        deep = Conv1D(256, 3, activation='relu')(deep)
        deep = GlobalAveragePooling1D()(deep)
        
        # åˆå¹¶
        merged = Concatenate()([wide, deep])
        output = Dense(128, activation='relu')(merged)
        output = Dropout(0.5)(output)
        output = Dense(6, activation='softmax')(output)
        
        model = Model(inputs, output)
        return model

class Model15_PyramidCNN:
    """é‡‘å­—å¡”CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(32, 7, activation='relu', input_shape=self.input_shape),
            MaxPooling1D(2),
            Conv1D(64, 5, activation='relu'),
            MaxPooling1D(2),
            Conv1D(128, 3, activation='relu'),
            MaxPooling1D(2),
            Conv1D(256, 3, activation='relu'),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model16_StackedLSTM:
    """å †å LSTMæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=self.input_shape),
            Dropout(0.3),
            LSTM(128, return_sequences=True),
            Dropout(0.3),
            LSTM(64, return_sequences=True),
            Dropout(0.3),
            LSTM(32),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model17_DilatedCNN:
    """æ‰©å¼ å·ç§¯æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 3, dilation_rate=1, activation='relu', padding='same', input_shape=self.input_shape),
            Conv1D(64, 3, dilation_rate=2, activation='relu', padding='same'),
            Conv1D(128, 3, dilation_rate=4, activation='relu', padding='same'),
            Conv1D(128, 3, dilation_rate=8, activation='relu', padding='same'),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model18_LocallyConnected:
    """å±€éƒ¨è¿æ¥æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            LocallyConnected1D(64, 3, activation='relu', input_shape=self.input_shape),
            Dropout(0.3),
            LocallyConnected1D(128, 3, activation='relu'),
            Dropout(0.3),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model19_ELU_CNN:
    """ELUæ¿€æ´»å‡½æ•°CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 3, activation='elu', input_shape=self.input_shape),
            BatchNormalization(),
            Conv1D(128, 3, activation='elu'),
            BatchNormalization(),
            MaxPooling1D(2),
            Conv1D(256, 3, activation='elu'),
            BatchNormalization(),
            GlobalAveragePooling1D(),
            Dense(128, activation='elu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model20_Swish_CNN:
    """Swishæ¿€æ´»å‡½æ•°CNNæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        def swish(x):
            return x * K.sigmoid(x)
        
        model = Sequential([
            Conv1D(64, 3, input_shape=self.input_shape),
            Lambda(swish),
            BatchNormalization(),
            Conv1D(128, 3),
            Lambda(swish),
            BatchNormalization(),
            MaxPooling1D(2),
            Conv1D(256, 3),
            Lambda(swish),
            GlobalAveragePooling1D(),
            Dense(128),
            Lambda(swish),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model21_L1Regularized:
    """L1æ­£åˆ™åŒ–æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 3, activation='relu', kernel_regularizer=l1(0.01), input_shape=self.input_shape),
            Conv1D(128, 3, activation='relu', kernel_regularizer=l1(0.01)),
            MaxPooling1D(2),
            Conv1D(256, 3, activation='relu', kernel_regularizer=l1(0.01)),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu', kernel_regularizer=l1(0.01)),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model22_L2Regularized:
    """L2æ­£åˆ™åŒ–æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 3, activation='relu', kernel_regularizer=l2(0.01), input_shape=self.input_shape),
            Conv1D(128, 3, activation='relu', kernel_regularizer=l2(0.01)),
            MaxPooling1D(2),
            Conv1D(256, 3, activation='relu', kernel_regularizer=l2(0.01)),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu', kernel_regularizer=l2(0.01)),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model23_DoubleConv:
    """åŒå·ç§¯æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(32, 3, activation='relu', input_shape=self.input_shape),
            Conv1D(32, 3, activation='relu'),
            MaxPooling1D(2),
            Conv1D(64, 3, activation='relu'),
            Conv1D(64, 3, activation='relu'),
            MaxPooling1D(2),
            Conv1D(128, 3, activation='relu'),
            Conv1D(128, 3, activation='relu'),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model24_GlobalMaxPool:
    """å…¨å±€æœ€å¤§æ± åŒ–æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 3, activation='relu', input_shape=self.input_shape),
            Conv1D(128, 3, activation='relu'),
            Conv1D(256, 3, activation='relu'),
            GlobalMaxPooling1D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model25_AvgPool:
    """å¹³å‡æ± åŒ–æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Conv1D(64, 3, activation='relu', input_shape=self.input_shape),
            AveragePooling1D(2),
            Conv1D(128, 3, activation='relu'),
            AveragePooling1D(2),
            Conv1D(256, 3, activation='relu'),
            GlobalAveragePooling1D(),
            Dense(128, activation='relu'),
            Dropout(0.5),
            Dense(6, activation='softmax')
        ])
        return model

class Model26_MambaInspired:
    """Mambaå¯å‘çš„é€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # ç‰¹å¾åµŒå…¥
        x = Conv1D(128, 1, activation='linear')(inputs)
        
        # é€‰æ‹©æ€§é—¨æ§æœºåˆ¶ (ç®€åŒ–ç‰ˆMambaæ ¸å¿ƒæ€æƒ³)
        # è¾“å…¥é—¨ï¼šå†³å®šå“ªäº›ä¿¡æ¯éœ€è¦æ›´æ–°
        input_gate = Dense(128, activation='sigmoid')(x)
        
        # é—å¿˜é—¨ï¼šå†³å®šå“ªäº›ä¿¡æ¯éœ€è¦é—å¿˜  
        forget_gate = Dense(128, activation='sigmoid')(x)
        
        # å€™é€‰çŠ¶æ€
        candidate = Dense(128, activation='tanh')(x)
        
        # çŠ¶æ€æ›´æ–° (ç±»ä¼¼LSTMä½†æ›´ç®€åŒ–)
        def selective_update(inputs):
            x, i_gate, f_gate, candidate = inputs
            # æ¨¡æ‹ŸçŠ¶æ€ç©ºé—´æ›´æ–°
            state = f_gate * x + i_gate * candidate
            return state
        
        state = Lambda(selective_update)([x, input_gate, forget_gate, candidate])
        
        # è¾“å‡ºé—¨
        output_gate = Dense(128, activation='sigmoid')(state)
        output = Lambda(lambda x: x[0] * K.tanh(x[1]))([output_gate, state])
        
        # æ—¶åºå»ºæ¨¡
        lstm_out = LSTM(64, return_sequences=True)(output)
        lstm_out = LSTM(32)(lstm_out)
        
        # åˆ†ç±»å™¨
        dense = Dense(128, activation='relu')(lstm_out)
        dropout = Dropout(0.5)(dense)
        outputs = Dense(6, activation='softmax')(dropout)
        
        model = Model(inputs, outputs)
        return model

class Model27_TransformerLike:
    """Transformeré£æ ¼æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # ä½ç½®ç¼–ç 
        x = Conv1D(128, 1)(inputs)
        
        # å¤šå¤´æ³¨æ„åŠ›
        attention = MultiHeadAttention(num_heads=8, key_dim=16)(x, x)
        attention = Dropout(0.1)(attention)
        
        # æ®‹å·®è¿æ¥
        x = Add()([x, attention])
        x = LayerNormalization()(x)
        
        # å‰é¦ˆç½‘ç»œ
        ff = Dense(256, activation='relu')(x)
        ff = Dropout(0.1)(ff)
        ff = Dense(128)(ff)
        
        # å†æ¬¡æ®‹å·®è¿æ¥
        x = Add()([x, ff])
        x = LayerNormalization()(x)
        
        # å…¨å±€æ± åŒ–
        x = GlobalAveragePooling1D()(x)
        
        # åˆ†ç±»å™¨
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model



class Model28_PureMamba_Fixed:
    """ä¿®å¤åçš„çº¯MambaçŠ¶æ€ç©ºé—´æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # è¾“å…¥æŠ•å½±
        x = Dense(128)(inputs)
        
        # ä½¿ç”¨è‡ªå®šä¹‰Mambaå—
        x = MambaBlock(d_model=128)(x)
        x = Dropout(0.2)(x)
        x = MambaBlock(d_model=128)(x)
        x = Dropout(0.2)(x)
        
        # è¾“å‡ºå±‚
        x = GlobalAveragePooling1D()(x)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model29_MambaTransformer:
    """Mamba+Transformeræ··åˆæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # Mambaåˆ†æ”¯
        mamba_x = Dense(64)(inputs)
        mamba_gate = Dense(64, activation='sigmoid')(mamba_x)
        mamba_out = Lambda(lambda t: t[0] * t[1])([mamba_x, mamba_gate])
        
        # Transformeråˆ†æ”¯
        trans_x = Dense(64)(inputs)
        attention = MultiHeadAttention(num_heads=4, key_dim=16)(trans_x, trans_x)
        trans_out = Add()([trans_x, attention])
        trans_out = LayerNormalization()(trans_out)
        
        # èåˆ
        merged = Concatenate()([mamba_out, trans_out])
        x = Dense(128, activation='relu')(merged)
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model30_ViTLike_Fixed:
    """ä¿®å¤åçš„Vision Transformeré£æ ¼1Dæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # Patch embedding - è°ƒæ•´patchå¤§å°ç¡®ä¿æ•´é™¤
        patch_size = 9  # 189å¯ä»¥è¢«9æ•´é™¤
        num_patches = self.input_shape[0] // patch_size  # 21
        
        # é‡å¡‘ä¸ºpatches
        x = Lambda(lambda t: K.reshape(t, (-1, num_patches, patch_size)))(inputs)
        x = Dense(128)(x)  # Patch embedding
        
        # ä½ç½®ç¼–ç  - ä½¿ç”¨è‡ªå®šä¹‰å±‚ï¼ˆè¿™é‡Œæ˜¯å…³é”®ä¿®æ”¹ï¼‰
        x = PositionalEncoding(num_patches=num_patches, d_model=128)(x)
        
        # Transformerå±‚
        for _ in range(3):
            # ä¿å­˜è¾“å…¥ç”¨äºæ®‹å·®è¿æ¥
            shortcut = x
            
            # å¤šå¤´æ³¨æ„åŠ›
            attention = MultiHeadAttention(num_heads=8, key_dim=16)(x, x)
            x = Add()([shortcut, attention])
            x = LayerNormalization()(x)
            
            # MLP
            shortcut = x
            mlp = Dense(256, activation='gelu')(x)
            mlp = Dense(128)(mlp)
            x = Add()([shortcut, mlp])
            x = LayerNormalization()(x)
        
        # åˆ†ç±»å¤´
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model31_BertLike:
    """BERTé£æ ¼ç¼–ç å™¨æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # è¾“å…¥åµŒå…¥
        x = Dense(256)(inputs)
        
        # BERTç¼–ç å™¨å±‚
        for i in range(4):
            # å¤šå¤´è‡ªæ³¨æ„åŠ›
            attention = MultiHeadAttention(
                num_heads=8, 
                key_dim=32,
                dropout=0.1
            )(x, x)
            attention = Dropout(0.1)(attention)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = Add()([x, attention])
            x = LayerNormalization()(x)
            
            # å‰é¦ˆç½‘ç»œ
            ff = Dense(512, activation='gelu')(x)
            ff = Dropout(0.1)(ff)
            ff = Dense(256)(ff)
            
            # æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–
            x = Add()([x, ff])
            x = LayerNormalization()(x)
        
        # æ± åŒ–å’Œåˆ†ç±»
        x = GlobalAveragePooling1D()(x)
        x = Dense(256, activation='gelu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model32_GPTLike:
    """GPTé£æ ¼è§£ç å™¨æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # è¾“å…¥åµŒå…¥
        x = Dense(256)(inputs)
        
        # GPTè§£ç å™¨å±‚ï¼ˆå› æœæ³¨æ„åŠ›ï¼‰
        for i in range(4):
            # å› æœæ³¨æ„åŠ›ï¼ˆç®€åŒ–ç‰ˆï¼‰
            attention = MultiHeadAttention(
                num_heads=8,
                key_dim=32,
                dropout=0.1
            )(x, x, use_causal_mask=True)
            attention = Dropout(0.1)(attention)
            
            # æ®‹å·®è¿æ¥
            x = Add()([x, attention])
            x = LayerNormalization()(x)
            
            # MLP
            mlp = Dense(1024, activation='gelu')(x)
            mlp = Dropout(0.1)(mlp)
            mlp = Dense(256)(mlp)
            
            x = Add()([x, mlp])
            x = LayerNormalization()(x)
        
        # æœ€åä¸€ä¸ªä½ç½®çš„è¾“å‡º
        x = Lambda(lambda t: t[:, -1, :])(x)
        x = Dense(256, activation='gelu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model33_LinearAttention_Fixed:
    """ä¿®å¤åçš„çº¿æ€§æ³¨æ„åŠ›æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # ç‰¹å¾æ˜ å°„
        x = Dense(128)(inputs)
        
        # ä½¿ç”¨è‡ªå®šä¹‰çº¿æ€§æ³¨æ„åŠ›å±‚
        x = LinearAttentionLayer(units=128)(x)
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model34_SparseAttention:
    """ç¨€ç–æ³¨æ„åŠ›æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # è¾“å…¥æŠ•å½±
        x = Dense(128)(inputs)
        
        # ç¨€ç–æ³¨æ„åŠ›ï¼ˆå±€éƒ¨çª—å£ï¼‰
        window_size = 16
        for i in range(3):
            # å±€éƒ¨è‡ªæ³¨æ„åŠ›
            attention = MultiHeadAttention(num_heads=4, key_dim=32)(x, x)
            x = Add()([x, attention])
            x = LayerNormalization()(x)
            
            # å‰é¦ˆ
            ff = Dense(256, activation='relu')(x)
            ff = Dense(128)(ff)
            x = Add()([x, ff])
            x = LayerNormalization()(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model35_CrossAttention_Fixed:
    """ä¿®å¤åçš„äº¤å‰æ³¨æ„åŠ›æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # ä¸¤ä¸ªå¹¶è¡Œåˆ†æ”¯ï¼Œä½¿ç”¨padding='same'ä¿æŒåºåˆ—é•¿åº¦ä¸€è‡´
        branch1 = Conv1D(64, 3, activation='relu', padding='same')(inputs)
        branch2 = Conv1D(64, 5, activation='relu', padding='same')(inputs)
        
        # äº¤å‰æ³¨æ„åŠ›
        cross_attn1 = MultiHeadAttention(num_heads=4, key_dim=16)(branch1, branch2)
        cross_attn2 = MultiHeadAttention(num_heads=4, key_dim=16)(branch2, branch1)
        
        # èåˆ
        merged = Concatenate()([cross_attn1, cross_attn2])
        x = Dense(128, activation='relu')(merged)
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model36_SelfAttention:
    """çº¯è‡ªæ³¨æ„åŠ›æ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # å¤šå±‚è‡ªæ³¨æ„åŠ›
        x = Dense(128)(inputs)
        
        for i in range(6):
            attention = MultiHeadAttention(
                num_heads=8,
                key_dim=16,
                dropout=0.1
            )(x, x)
            x = Add()([x, attention])
            x = LayerNormalization()(x)
            x = Dropout(0.1)(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model37_HierarchicalTransformer:
    """å±‚æ¬¡åŒ–Transformeræ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # ç¬¬ä¸€å±‚ï¼šå±€éƒ¨ç‰¹å¾
        x = Dense(64)(inputs)
        local_attn = MultiHeadAttention(num_heads=4, key_dim=16)(x, x)
        x = Add()([x, local_attn])
        x = LayerNormalization()(x)
        
        # ä¸‹é‡‡æ ·
        x = MaxPooling1D(2)(x)
        
        # ç¬¬äºŒå±‚ï¼šä¸­å±‚ç‰¹å¾
        x = Dense(128)(x)
        mid_attn = MultiHeadAttention(num_heads=8, key_dim=16)(x, x)
        x = Add()([x, mid_attn])
        x = LayerNormalization()(x)
        
        # å†æ¬¡ä¸‹é‡‡æ ·
        x = MaxPooling1D(2)(x)
        
        # ç¬¬ä¸‰å±‚ï¼šå…¨å±€ç‰¹å¾
        x = Dense(256)(x)
        global_attn = MultiHeadAttention(num_heads=8, key_dim=32)(x, x)
        x = Add()([x, global_attn])
        x = LayerNormalization()(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model38_ConvTransformer:
    """å·ç§¯Transformeræ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # å·ç§¯ç‰¹å¾æå–
        x = Conv1D(64, 7, activation='relu', padding='same')(inputs)
        x = Conv1D(128, 5, activation='relu', padding='same')(x)
        x = MaxPooling1D(2)(x)
        
        # Transformerå±‚
        for i in range(3):
            attention = MultiHeadAttention(num_heads=8, key_dim=16)(x, x)
            x = Add()([x, attention])
            x = LayerNormalization()(x)
            
            # å·ç§¯å‰é¦ˆç½‘ç»œ
            ff = Conv1D(256, 1, activation='relu')(x)
            ff = Conv1D(128, 1)(ff)
            x = Add()([x, ff])
            x = LayerNormalization()(x)
        
        x = GlobalAveragePooling1D()(x)
        x = Dense(128, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

class Model39_MegaLSTM:
    """è¶…å¤§LSTMæ¨¡å‹"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        model = Sequential([
            Bidirectional(LSTM(256, return_sequences=True), input_shape=self.input_shape),
            Dropout(0.3),
            Bidirectional(LSTM(256, return_sequences=True)),
            Dropout(0.3),
            Bidirectional(LSTM(128, return_sequences=True)),
            Dropout(0.3),
            Bidirectional(LSTM(128, return_sequences=True)),
            Dropout(0.3),
            Bidirectional(LSTM(64)),
            Dense(512, activation='relu'),
            Dropout(0.5),
            Dense(256, activation='relu'),
            Dropout(0.3),
            Dense(6, activation='softmax')
        ])
        return model

class Model40_HybridMamba_Fixed:
    """ä¿®å¤åçš„æ··åˆMambaæ¶æ„"""
    def __init__(self, input_shape=(189, 1)):
        self.input_shape = input_shape
        
    def build(self):
        inputs = Input(shape=self.input_shape)
        
        # CNNåˆ†æ”¯ï¼Œä½¿ç”¨padding='same'ä¿æŒåºåˆ—é•¿åº¦
        cnn_branch = Conv1D(64, 3, activation='relu', padding='same')(inputs)
        cnn_branch = Conv1D(128, 3, activation='relu', padding='same')(cnn_branch)
        
        # Mambaåˆ†æ”¯
        mamba_x = Dense(128)(inputs)
        mamba_gate = Dense(128, activation='sigmoid')(mamba_x)
        mamba_forget = Dense(128, activation='sigmoid')(mamba_x)
        mamba_candidate = Dense(128, activation='tanh')(mamba_x)
        
        # MambaçŠ¶æ€æ›´æ–°
        mamba_state = Lambda(lambda t: t[0] * t[1] + t[2] * t[3])([
            mamba_forget, mamba_x, mamba_gate, mamba_candidate
        ])
        
        # LSTMåˆ†æ”¯
        lstm_branch = LSTM(128, return_sequences=True)(inputs)
        
        # ç¡®ä¿æ‰€æœ‰åˆ†æ”¯å½¢çŠ¶ä¸€è‡´
        # cnn_branch: (None, 189, 128)
        # mamba_state: (None, 189, 128)
        # lstm_branch: (None, 189, 128)
        
        # æ³¨æ„åŠ›èåˆ
        concat_features = Concatenate()([cnn_branch, mamba_state, lstm_branch])
        attention_weights = Dense(1, activation='softmax')(concat_features)
        
        # åŠ æƒèåˆ
        weighted_cnn = Lambda(lambda t: t[0] * t[1][:, :, 0:1])([cnn_branch, attention_weights])
        weighted_mamba = Lambda(lambda t: t[0] * t[1][:, :, 0:1])([mamba_state, attention_weights])
        weighted_lstm = Lambda(lambda t: t[0] * t[1][:, :, 0:1])([lstm_branch, attention_weights])
        
        # æœ€ç»ˆèåˆ
        final_output = Add()([weighted_cnn, weighted_mamba, weighted_lstm])
        x = GlobalAveragePooling1D()(final_output)
        x = Dense(256, activation='relu')(x)
        x = Dropout(0.5)(x)
        outputs = Dense(6, activation='softmax')(x)
        
        model = Model(inputs, outputs)
        return model

# åˆ›å»ºæ‰€æœ‰æ¨¡å‹å®ä¾‹ï¼ˆåŒ…å«ä¿®å¤åçš„æ¨¡å‹ï¼‰
models = [
    Model1_BasicCNN(), Model2_DeepCNN(), Model3_BatchNormCNN(), Model4_LSTM(),
    Model5_BiLSTM(), Model6_GRU(), Model7_CNN_LSTM(), Model8_CNN_GRU(),
    Model9_ResidualCNN(), Model10_MultiScale(), Model11_DenseCNN(), Model12_SeparableCNN(),
    Model13_AttentionLSTM(), Model14_WideDeepCNN(), Model15_PyramidCNN(), Model16_StackedLSTM(),
    Model17_DilatedCNN(), Model18_LocallyConnected(), Model19_ELU_CNN(), Model20_Swish_CNN(),
    Model21_L1Regularized(), Model22_L2Regularized(), Model23_DoubleConv(), Model24_GlobalMaxPool(),
    Model25_AvgPool(), Model26_MambaInspired(), Model27_TransformerLike(), 
    Model28_PureMamba_Fixed(),  
    Model29_MambaTransformer(), 
    Model30_ViTLike_Fixed(), 
    Model31_BertLike(), Model32_GPTLike(),
    Model33_LinearAttention_Fixed(), 
    Model34_SparseAttention(), 
    Model35_CrossAttention_Fixed(),  
    Model36_SelfAttention(),
    Model37_HierarchicalTransformer(), Model38_ConvTransformer(), Model39_MegaLSTM(), 
    Model40_HybridMamba_Fixed() 
]

# è®­ç»ƒå¹¶ä¿å­˜æ‰€æœ‰æ¨¡å‹
model_names = [
    # 'basic_cnn', 'deep_cnn', 'batch_norm_cnn', 'lstm', 'bilstm', 'gru', 
    # 'cnn_lstm', 'cnn_gru', 'residual_cnn', 'multiscale', 'dense_cnn', 
    # 'separable_cnn', 'attention_lstm', 'wide_deep_cnn', 'pyramid_cnn', 
    # 'stacked_lstm', 'dilated_cnn', 'locally_connected', 'elu_cnn', 
    # 'swish_cnn', 'l1_regularized', 'l2_regularized', 'double_conv', 
    # 'global_max_pool', 'avg_pool', 'mamba_inspired', 'transformer_like',
    # 'pure_mamba', 'mamba_transformer', 'vit_like', 'bert_like', 'gpt_like',
    # 'linear_attention', 'sparse_attention', 'cross_attention', 'self_attention',
    # 'hierarchical_transformer', 'conv_transformer', 'mega_lstm', 'hybrid_mamba'
    'vit_like'
]

# è‡ªå®šä¹‰è¿›åº¦æ¡å›è°ƒ
class CustomTqdmCallback(Callback):
    def __init__(self, model_name, total_epochs):
        super().__init__()
        self.model_name = model_name
        self.total_epochs = total_epochs
        self.pbar = None
        
    def on_train_begin(self, logs=None):
        self.pbar = tqdm(total=self.total_epochs, desc=f"ğŸ”¥ è®­ç»ƒ {self.model_name}", 
                        unit="epoch", ncols=150, leave=False, 
                        bar_format='{l_bar}{bar:50}{r_bar}{bar:-50b}')
        
    def on_epoch_end(self, epoch, logs=None):
        if logs:
            self.pbar.set_postfix({
                'ğŸ“‰ loss': f"{logs.get('loss', 0):.4f}",
                'ğŸ“ˆ acc': f"{logs.get('accuracy', 0):.4f}",
                'ğŸ“‰ val_loss': f"{logs.get('val_loss', 0):.4f}",
                'ğŸ“ˆ val_acc': f"{logs.get('val_accuracy', 0):.4f}"
            })
        self.pbar.update(1)
        
    def on_train_end(self, logs=None):
        if self.pbar:
            self.pbar.close()

print("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€æœ‰æ¨¡å‹...")
print("=" * 60)

# è®°å½•è®­ç»ƒç»“æœ
training_results = []
total_models = len(models)

# æ€»ä½“è¿›åº¦æ¡
overall_pbar = tqdm(total=total_models, desc="ğŸš€ æ€»ä½“è®­ç»ƒè¿›åº¦", position=0, leave=True, 
                   ncols=120, bar_format='{l_bar}{bar:40}{r_bar}{bar:-40b}',
                   colour='green')

for i, (model_class, model_name) in enumerate(zip(models, model_names)):
    start_time = time.time()
    
    print(f"\n" + "="*80)
    print(f"ğŸ”¥ æ­£åœ¨è®­ç»ƒæ¨¡å‹ {i+1}/{total_models}: {model_name.upper()}")
    print("="*80)
    
    try:
        # æ„å»ºæ¨¡å‹
        model = model_class.build()
        
        # ç¼–è¯‘æ¨¡å‹
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        
        # æ˜¾ç¤ºæ¨¡å‹å‚æ•°æ•°é‡
        total_params = model.count_params()
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {total_params:,}")
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%H:%M:%S')}")
        print("-" * 80)
        
        # åˆ›å»ºè¿›åº¦æ¡å›è°ƒ
        progress_callback = CustomTqdmCallback(model_name, 125)
        
        # è®­ç»ƒæ¨¡å‹
        history = model.fit(
            X_train_reshaped, y_train, 
            batch_size=32, 
            epochs=125, 
            validation_split=0.25, 
            verbose=0,  # å…³é—­é»˜è®¤è¾“å‡º
            callbacks=[progress_callback]
        )
        
        # è¯„ä¼°æ¨¡å‹
        test_loss, test_acc = model.evaluate(X_test_reshaped, y_test, verbose=0)
        
        # ä¿å­˜æ¨¡å‹ï¼ˆä½¿ç”¨æ–°çš„.kerasæ ¼å¼ä»¥é¿å…è­¦å‘Šï¼‰
        model.save(f'./Model/{model_name}.keras')
        
        # è®°å½•ç»“æœ
        training_time = time.time() - start_time
        result = {
            'model_name': model_name,
            'test_accuracy': test_acc,
            'test_loss': test_loss,
            'total_params': total_params,
            'training_time': training_time
        }
        training_results.append(result)
        
        print("-" * 80)
        print(f"âœ… æ¨¡å‹ {model_name} è®­ç»ƒå®Œæˆ!")
        print(f"   ğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"   ğŸ“‰ æµ‹è¯•æŸå¤±: {test_loss:.4f}")
        print(f"   â±ï¸  è®­ç»ƒæ—¶é—´: {training_time:.1f}ç§’")
        print(f"   ğŸ’¾ å·²ä¿å­˜åˆ°: ./Model/{model_name}.keras")
        print("=" * 80)
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹ {model_name} è®­ç»ƒå¤±è´¥: {str(e)}")
        result = {
            'model_name': model_name,
            'test_accuracy': 0,
            'test_loss': float('inf'),
            'total_params': 0,
            'training_time': 0,
            'error': str(e)
        }
        training_results.append(result)
        
    finally:
        overall_pbar.update(1)

overall_pbar.close()

print("\n" + "ğŸ‰" * 30)
print("ğŸ† æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆ! ğŸ†")
print("ğŸ‰" * 30)

# æ˜¾ç¤ºè®­ç»ƒç»“æœæ‘˜è¦
print("\nğŸ“Š è®­ç»ƒç»“æœæ’è¡Œæ¦œ:")
print("=" * 120)
print(f"{'æ’å':<6} {'æ¨¡å‹åç§°':<25} {'ğŸ¯å‡†ç¡®ç‡':<12} {'ğŸ“‰æŸå¤±':<12} {'ğŸ“Šå‚æ•°é‡':<15} {'â±ï¸è®­ç»ƒæ—¶é—´(s)':<15}")
print("=" * 120)

# æŒ‰å‡†ç¡®ç‡æ’åº
successful_results = [r for r in training_results if 'error' not in r]
successful_results.sort(key=lambda x: x['test_accuracy'], reverse=True)

for idx, result in enumerate(successful_results[:40], 1):  # æ˜¾ç¤ºæ‰€æœ‰æˆåŠŸçš„æ¨¡å‹
    rank_emoji = "ğŸ¥‡" if idx == 1 else "ğŸ¥ˆ" if idx == 2 else "ğŸ¥‰" if idx == 3 else f"{idx:2d}"
    print(f"{rank_emoji:<6} {result['model_name']:<25} {result['test_accuracy']:<12.4f} "
          f"{result['test_loss']:<12.4f} {result['total_params']:<15,} "
          f"{result['training_time']:<15.1f}")

# æ˜¾ç¤ºå¤±è´¥çš„æ¨¡å‹
failed_results = [r for r in training_results if 'error' in r]
if failed_results:
    print(f"\nâŒ å¤±è´¥çš„æ¨¡å‹ ({len(failed_results)}ä¸ª):")
    for result in failed_results:
        print(f"   {result['model_name']}: {result['error']}")

print(f"\nğŸ“ å·²ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ ({len(successful_results)}ä¸ª):")
for result in successful_results:
    print(f"   ./Model/{result['model_name']}.keras")

if successful_results:
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {successful_results[0]['model_name']} (å‡†ç¡®ç‡: {successful_results[0]['test_accuracy']:.4f})")
    print("ğŸŠ" * 50)
