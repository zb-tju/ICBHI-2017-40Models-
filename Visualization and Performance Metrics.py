import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import load_model
from keras.layers import Layer, Lambda
from sklearn.metrics import (confusion_matrix, classification_report, 
                           roc_curve, auc, precision_recall_curve,
                           accuracy_score, precision_score, recall_score, 
                           f1_score, cohen_kappa_score, matthews_corrcoef)
from sklearn.preprocessing import LabelEncoder, label_binarize
from keras.utils import to_categorical
import keras.backend as K
import warnings
import os
from tqdm import tqdm
import time
from keras.layers import Dense

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['Noto Sans CJK JP']
plt.rcParams['axes.unicode_minus'] = False


class MambaBlock(Layer):
    def __init__(self, d_model=128, **kwargs):
        super(MambaBlock, self).__init__(**kwargs)
        self.d_model = d_model
        
    def build(self, input_shape):
        from keras.layers import Dense
        super(MambaBlock, self).build(input_shape)
        self.dense_proj = Dense(self.d_model * 2)
        self.dense_delta = Dense(self.d_model, activation='softplus')
        self.dense_A = Dense(self.d_model, activation='sigmoid')
        self.dense_B = Dense(self.d_model)
        self.dense_C = Dense(self.d_model)
        
    def call(self, inputs):
        x_proj = self.dense_proj(inputs)
        x = x_proj[..., :self.d_model]
        z = x_proj[..., self.d_model:]
        
        delta = self.dense_delta(x)
        A = self.dense_A(x)
        B = self.dense_B(x)
        C = self.dense_C(x)
        
        # ç®€åŒ–çš„SSMè®¡ç®—
        h = x * A + B
        y = C * h
        
        # é—¨æ§
        y = y * K.sigmoid(z)
        return y
    
    def get_config(self):
        config = super(MambaBlock, self).get_config()
        config.update({"d_model": self.d_model})
        return config
    
class LinearAttentionLayer(Layer):
    """ä¿®å¤åçš„çº¿æ€§æ³¨æ„åŠ›å±‚ - æ•°å€¼ç¨³å®šç‰ˆæœ¬"""
    def __init__(self, units=128, **kwargs):
        super(LinearAttentionLayer, self).__init__(**kwargs)
        self.units = units
        
    def build(self, input_shape):
        super(LinearAttentionLayer, self).build(input_shape)
        self.query_dense = Dense(self.units)
        self.key_dense = Dense(self.units)
        self.value_dense = Dense(self.units)
        
    def call(self, inputs):
        q = self.query_dense(inputs)
        k = self.key_dense(inputs)
        v = self.value_dense(inputs)
        
        # æ•°å€¼ç¨³å®šçš„çº¿æ€§åŒ– - é¿å…expæº¢å‡º
        # æ–¹æ³•1: ä½¿ç”¨softplusæ›¿ä»£exp
        q = K.softplus(q) + 1e-8  # æ·»åŠ å°å¸¸æ•°é¿å…é›¶
        k = K.softplus(k) + 1e-8
        
        # æˆ–è€…æ–¹æ³•2: ä½¿ç”¨ReLU + 1
        # q = K.relu(q) + 1.0
        # k = K.relu(k) + 1.0
        
        # è®¡ç®—æ³¨æ„åŠ›
        kv = K.batch_dot(k, v, axes=[1, 1])
        qkv = K.batch_dot(q, kv, axes=[2, 1])
        
        return qkv
    
    def get_config(self):
        config = super(LinearAttentionLayer, self).get_config()
        config.update({"units": self.units})
        return config

class PositionalEncoding(Layer):
    """ä½ç½®ç¼–ç å±‚"""
    def __init__(self, num_patches, d_model, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.num_patches = num_patches
        self.d_model = d_model
        
    def build(self, input_shape):
        super(PositionalEncoding, self).build(input_shape)
        # åˆ›å»ºå¯å­¦ä¹ çš„ä½ç½®ç¼–ç 
        self.pos_embed = self.add_weight(
            name='pos_embed',
            shape=(1, self.num_patches, self.d_model),
            initializer='zeros',
            trainable=True
        )
        
    def call(self, inputs):
        return inputs + self.pos_embed
    
    def get_config(self):
        config = super(PositionalEncoding, self).get_config()
        config.update({
            "num_patches": self.num_patches,
            "d_model": self.d_model
        })
        return config

# å®šä¹‰è‡ªå®šä¹‰å¯¹è±¡å­—å…¸
custom_objects = {
    'MambaBlock': MambaBlock,
    'LinearAttentionLayer': LinearAttentionLayer,
    'PositionalEncoding': PositionalEncoding,
}

# åˆ›å»ºä¿å­˜ç»“æœçš„ç›®å½•
os.makedirs('./Evaluation_Results', exist_ok=True)
os.makedirs('./Evaluation_Results/ROC_Curves', exist_ok=True)
os.makedirs('./Evaluation_Results/Confusion_Matrices', exist_ok=True)
os.makedirs('./Evaluation_Results/PR_Curves', exist_ok=True)
os.makedirs('./Evaluation_Results/Performance_Comparison', exist_ok=True)

# åŠ è½½æ•°æ®
print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®...")
file_path = './Data/data.pkl'
with open(file_path, 'rb') as f:
    data = pickle.load(f)
    
X = np.array(list(zip(*data))[0])
y = np.array(list(zip(*data))[1])

# åº”ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ•°æ®å¤„ç†
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

sampling_strategy_over = {'Pneumonia': 150, 'Healthy': 150, 'URTI': 100, 
                         'Bronchiectasis': 100, 'Bronchiolitis': 100}
smote = SMOTE(sampling_strategy=sampling_strategy_over, k_neighbors=5, random_state=42)
X_resampled_over, y_resampled_over = smote.fit_resample(X, y)

copd_indices = np.where(y_resampled_over == 'COPD')[0]
np.random.seed(42)
np.random.shuffle(copd_indices)
remove_indices = copd_indices[:len(copd_indices) // 2]

X_resampled = np.delete(X_resampled_over, remove_indices, axis=0)
y_resampled = np.delete(y_resampled_over, remove_indices)

# ç¼–ç æ ‡ç­¾
le = LabelEncoder()
y_encoded = le.fit_transform(y_resampled)
y_categorical = to_categorical(y_encoded)
class_names = le.classes_

# åˆ†å‰²æ•°æ®é›†
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_categorical, 
                                                    test_size=0.2, random_state=42)
X_test_reshaped = np.reshape(X_test, (X_test.shape[0], 189, 1))

# è·å–çœŸå®æ ‡ç­¾ï¼ˆéone-hotï¼‰
y_test_labels = np.argmax(y_test, axis=1)

# æ¨¡å‹åç§°åˆ—è¡¨
model_names = [
    'basic_cnn', 'deep_cnn', 'batch_norm_cnn', 'lstm', 'bilstm', 'gru', 
    'cnn_lstm', 'cnn_gru', 'residual_cnn', 'multiscale', 'dense_cnn', 
    'separable_cnn', 'attention_lstm', 'wide_deep_cnn', 'pyramid_cnn', 
    'stacked_lstm', 'dilated_cnn', 'locally_connected', 'elu_cnn', 
    'swish_cnn', 'l1_regularized', 'l2_regularized', 'double_conv', 
    'global_max_pool', 'avg_pool', 'mamba_inspired', 'transformer_like',
    'pure_mamba', 'mamba_transformer', 'vit_like', 'bert_like', 'gpt_like',
    'sparse_attention', 'cross_attention', 'self_attention',
    'hierarchical_transformer', 'conv_transformer', 'mega_lstm', 'hybrid_mamba'
]

def plot_confusion_matrix(y_true, y_pred, model_name, class_names):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    cm = confusion_matrix(y_true, y_pred)
    
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'æ··æ·†çŸ©é˜µ - {model_name}', fontsize=16, pad=20)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.tight_layout()
    plt.savefig(f'./Evaluation_Results/Confusion_Matrices/{model_name}_confusion_matrix.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # è®¡ç®—å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', 
                xticklabels=class_names, yticklabels=class_names)
    plt.title(f'å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ - {model_name}', fontsize=16, pad=20)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.tight_layout()
    plt.savefig(f'./Evaluation_Results/Confusion_Matrices/{model_name}_normalized_confusion_matrix.png', 
                dpi=300, bbox_inches='tight')
    plt.close()

def plot_roc_curves(y_true, y_score, model_name, class_names):
    """ç»˜åˆ¶å¤šåˆ†ç±»ROCæ›²çº¿"""
    n_classes = len(class_names)
    
    # å°†æ ‡ç­¾äºŒå€¼åŒ–
    y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))
    
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ROCæ›²çº¿å’ŒAUC
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_true_bin[:, i], y_score[:, i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    
    # è®¡ç®—å¾®å¹³å‡ROCæ›²çº¿å’ŒAUC
    fpr["micro"], tpr["micro"], _ = roc_curve(y_true_bin.ravel(), y_score.ravel())
    roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])
    
    # è®¡ç®—å®å¹³å‡ROCæ›²çº¿å’ŒAUC
    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
    mean_tpr = np.zeros_like(all_fpr)
    for i in range(n_classes):
        mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])
    mean_tpr /= n_classes
    fpr["macro"] = all_fpr
    tpr["macro"] = mean_tpr
    roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])
    
    # ç»˜åˆ¶æ‰€æœ‰ROCæ›²çº¿
    plt.figure(figsize=(12, 10))
    
    # ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„ROCæ›²çº¿
    colors = plt.cm.Set3(np.linspace(0, 1, n_classes))
    for i, color in enumerate(colors):
        plt.plot(fpr[i], tpr[i], color=color, lw=2,
                label=f'{class_names[i]} (AUC = {roc_auc[i]:.3f})')
    
    # ç»˜åˆ¶å¾®å¹³å‡å’Œå®å¹³å‡
    plt.plot(fpr["micro"], tpr["micro"],
            label=f'å¾®å¹³å‡ (AUC = {roc_auc["micro"]:.3f})',
            color='deeppink', linestyle=':', linewidth=3)
    
    plt.plot(fpr["macro"], tpr["macro"],
            label=f'å®å¹³å‡ (AUC = {roc_auc["macro"]:.3f})',
            color='navy', linestyle=':', linewidth=3)
    
    # ç»˜åˆ¶å¯¹è§’çº¿
    plt.plot([0, 1], [0, 1], 'k--', lw=2, label='éšæœºåˆ†ç±»å™¨')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡é˜³æ€§ç‡ (FPR)', fontsize=12)
    plt.ylabel('çœŸé˜³æ€§ç‡ (TPR)', fontsize=12)
    plt.title(f'ROCæ›²çº¿ - {model_name}', fontsize=16, pad=20)
    plt.legend(loc="lower right", fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'./Evaluation_Results/ROC_Curves/{model_name}_roc_curves.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    return roc_auc

def plot_pr_curves(y_true, y_score, model_name, class_names):
    """ç»˜åˆ¶å¤šåˆ†ç±»PRæ›²çº¿"""
    n_classes = len(class_names)
    y_true_bin = label_binarize(y_true, classes=np.arange(n_classes))
    
    precision = dict()
    recall = dict()
    pr_auc = dict()
    
    for i in range(n_classes):
        precision[i], recall[i], _ = precision_recall_curve(y_true_bin[:, i], y_score[:, i])
        pr_auc[i] = auc(recall[i], precision[i])
    
    # ç»˜åˆ¶PRæ›²çº¿
    plt.figure(figsize=(12, 10))
    colors = plt.cm.Set3(np.linspace(0, 1, n_classes))
    
    for i, color in enumerate(colors):
        plt.plot(recall[i], precision[i], color=color, lw=2,
                label=f'{class_names[i]} (AUC = {pr_auc[i]:.3f})')
    
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å¬å›ç‡ (Recall)', fontsize=12)
    plt.ylabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12)
    plt.title(f'PRæ›²çº¿ - {model_name}', fontsize=16, pad=20)
    plt.legend(loc="lower left", fontsize=10)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(f'./Evaluation_Results/PR_Curves/{model_name}_pr_curves.png', 
                dpi=300, bbox_inches='tight')
    plt.close()

def calculate_metrics(y_true, y_pred, y_score):
    """è®¡ç®—å„ç§æ€§èƒ½æŒ‡æ ‡"""
    # åŸºç¡€æŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision_macro = precision_score(y_true, y_pred, average='macro')
    precision_weighted = precision_score(y_true, y_pred, average='weighted')
    recall_macro = recall_score(y_true, y_pred, average='macro')
    recall_weighted = recall_score(y_true, y_pred, average='weighted')
    f1_macro = f1_score(y_true, y_pred, average='macro')
    f1_weighted = f1_score(y_true, y_pred, average='weighted')
    
    # é«˜çº§æŒ‡æ ‡
    kappa = cohen_kappa_score(y_true, y_pred)
    mcc = matthews_corrcoef(y_true, y_pred)
    
    # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    class_report = classification_report(y_true, y_pred, output_dict=True)
    
    return {
        'accuracy': accuracy,
        'precision_macro': precision_macro,
        'precision_weighted': precision_weighted,
        'recall_macro': recall_macro,
        'recall_weighted': recall_weighted,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted,
        'cohen_kappa': kappa,
        'matthews_corrcoef': mcc,
        'class_report': class_report
    }

def load_model_safely(model_path, model_name):
    """å®‰å…¨åŠ è½½æ¨¡å‹ï¼Œå¤„ç†Lambdaå±‚å’Œè‡ªå®šä¹‰å±‚"""
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨safe_mode=FalseåŠ è½½ï¼ˆå¤„ç†Lambdaå±‚ï¼‰
        model = load_model(model_path, custom_objects=custom_objects, safe_mode=False)
        return model
    except Exception as e1:
        try:
            # å¦‚æœå¤±è´¥ï¼Œå°è¯•ä»…ä½¿ç”¨custom_objectsåŠ è½½
            model = load_model(model_path, custom_objects=custom_objects)
            return model
        except Exception as e2:
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼Œæ ¹æ®æ¨¡å‹åç§°æ·»åŠ ç‰¹å®šçš„Lambdaå‡½æ•°
            if 'swish' in model_name:
                def swish(x):
                    return x * K.sigmoid(x)
                custom_objects['swish'] = swish
            
            try:
                model = load_model(model_path, custom_objects=custom_objects, safe_mode=False)
                return model
            except Exception as e3:
                raise Exception(f"æ— æ³•åŠ è½½æ¨¡å‹ {model_name}: {str(e3)}")

# å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°ç»“æœ
all_results = []

print("\nğŸš€ å¼€å§‹è¯„ä¼°æ‰€æœ‰æ¨¡å‹...")
print("="*80)

# è¯„ä¼°æ¯ä¸ªæ¨¡å‹
for model_name in tqdm(model_names, desc="è¯„ä¼°è¿›åº¦"):
    model_path = f'./Model/{model_name}.keras'
    
    if not os.path.exists(model_path):
        print(f"\nâš ï¸  æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        continue
    
    try:
        print(f"\nğŸ“Š æ­£åœ¨è¯„ä¼°æ¨¡å‹: {model_name}")
        
        # å®‰å…¨åŠ è½½æ¨¡å‹
        model = load_model_safely(model_path, model_name)
        
        # é¢„æµ‹
        start_time = time.time()
        y_pred_proba = model.predict(X_test_reshaped, verbose=0)
        inference_time = time.time() - start_time
        
        y_pred = np.argmax(y_pred_proba, axis=1)
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        metrics = calculate_metrics(y_test_labels, y_pred, y_pred_proba)
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        plot_confusion_matrix(y_test_labels, y_pred, model_name, class_names)
        
        # ç»˜åˆ¶ROCæ›²çº¿
        roc_auc_scores = plot_roc_curves(y_test_labels, y_pred_proba, model_name, class_names)
        
        # ç»˜åˆ¶PRæ›²çº¿
        plot_pr_curves(y_test_labels, y_pred_proba, model_name, class_names)
        
        # æ•´ç†ç»“æœ
        result = {
            'model_name': model_name,
            'accuracy': metrics['accuracy'],
            'precision_macro': metrics['precision_macro'],
            'precision_weighted': metrics['precision_weighted'],
            'recall_macro': metrics['recall_macro'],
            'recall_weighted': metrics['recall_weighted'],
            'f1_macro': metrics['f1_macro'],
            'f1_weighted': metrics['f1_weighted'],
            'cohen_kappa': metrics['cohen_kappa'],
            'matthews_corrcoef': metrics['matthews_corrcoef'],
            'auc_macro': roc_auc_scores['macro'],
            'auc_micro': roc_auc_scores['micro'],
            'inference_time_total': inference_time,
            'inference_time_per_sample': inference_time / len(X_test_reshaped),
            'total_params': model.count_params()
        }
        
        # æ·»åŠ æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
        for class_name in class_names:
            if class_name in metrics['class_report']:
                result[f'{class_name}_precision'] = metrics['class_report'][class_name]['precision']
                result[f'{class_name}_recall'] = metrics['class_report'][class_name]['recall']
                result[f'{class_name}_f1-score'] = metrics['class_report'][class_name]['f1-score']
        
        all_results.append(result)
        
        print(f"âœ… {model_name} è¯„ä¼°å®Œæˆ: Accuracy={metrics['accuracy']:.4f}, AUC={roc_auc_scores['macro']:.4f}")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°æ¨¡å‹ {model_name} æ—¶å‡ºé”™: {str(e)}")
        # è®°å½•å¤±è´¥çš„æ¨¡å‹
        result = {
            'model_name': model_name,
            'error': str(e),
            'accuracy': 0,
            'auc_macro': 0
        }
        all_results.append(result)

# ä¿å­˜è¯„ä¼°ç»“æœåˆ°CSV
if all_results:
    # ç­›é€‰å‡ºæˆåŠŸè¯„ä¼°çš„ç»“æœ
    successful_results = [r for r in all_results if 'error' not in r]
    
    if successful_results:
        df_results = pd.DataFrame(successful_results)
        df_results = df_results.sort_values('accuracy', ascending=False)
        df_results.to_csv('./Evaluation_Results/model_evaluation_results.csv', index=False)
        print(f"\nğŸ’¾ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: ./Evaluation_Results/model_evaluation_results.csv")
        
        # æ˜¾ç¤ºå‰10ä¸ªæ¨¡å‹çš„ç»“æœ
        print("\nğŸ† Top 10 æ¨¡å‹æ€§èƒ½æ’å:")
        print("="*100)
        print(df_results[['model_name', 'accuracy', 'f1_weighted', 'auc_macro', 
                         'cohen_kappa', 'inference_time_per_sample']].head(10).to_string(index=False))

# ç»˜åˆ¶æ€§èƒ½æ¯”è¾ƒå›¾
def plot_performance_comparison(df_results):
    """ç»˜åˆ¶æ¨¡å‹æ€§èƒ½æ¯”è¾ƒå›¾"""
    
    # 1. å‡†ç¡®ç‡å¯¹æ¯”æ¡å½¢å›¾
    plt.figure(figsize=(15, 10))
    df_sorted = df_results.sort_values('accuracy', ascending=True)
    colors = plt.cm.viridis(np.linspace(0, 1, len(df_sorted)))
    
    plt.barh(df_sorted['model_name'], df_sorted['accuracy'], color=colors)
    plt.xlabel('å‡†ç¡®ç‡', fontsize=12)
    plt.title('æ¨¡å‹å‡†ç¡®ç‡å¯¹æ¯”', fontsize=16, pad=20)
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig('./Evaluation_Results/Performance_Comparison/accuracy_comparison.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. å¤šæŒ‡æ ‡é›·è¾¾å›¾ï¼ˆå‰5ä¸ªæ¨¡å‹ï¼‰
    top_models = df_results.head(5)
    categories = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC']
    
    fig = plt.figure(figsize=(12, 10))
    ax = fig.add_subplot(111, projection='polar')
    
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]
    
    for idx, row in top_models.iterrows():
        values = [
            row['accuracy'],
            row['precision_weighted'],
            row['recall_weighted'],
            row['f1_weighted'],
            row['auc_macro']
        ]
        values += values[:1]
        
        ax.plot(angles, values, 'o-', linewidth=2, label=row['model_name'])
        ax.fill(angles, values, alpha=0.15)
    
    ax.set_theta_offset(np.pi / 2)
    ax.set_theta_direction(-1)
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories)
    ax.set_ylim(0, 1)
    ax.set_title('Top 5 æ¨¡å‹å¤šç»´åº¦æ€§èƒ½å¯¹æ¯”', fontsize=16, pad=30)
    ax.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))
    ax.grid(True)
    
    plt.tight_layout()
    plt.savefig('./Evaluation_Results/Performance_Comparison/radar_chart_top5.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # 3. æ€§èƒ½vsæ¨ç†æ—¶é—´æ•£ç‚¹å›¾
    plt.figure(figsize=(12, 8))
    scatter = plt.scatter(df_results['inference_time_per_sample'] * 1000, 
                         df_results['accuracy'],
                         c=df_results['total_params'],
                         s=100, alpha=0.6, cmap='coolwarm')
    
    plt.xlabel('æ¨ç†æ—¶é—´ (ms/æ ·æœ¬)', fontsize=12)
    plt.ylabel('å‡†ç¡®ç‡', fontsize=12)
    plt.title('æ¨¡å‹å‡†ç¡®ç‡ vs æ¨ç†æ—¶é—´', fontsize=16, pad=20)
    
    # æ·»åŠ é¢œè‰²æ¡
    cbar = plt.colorbar(scatter)
    cbar.set_label('æ¨¡å‹å‚æ•°é‡', fontsize=10)
    
    # æ ‡æ³¨å‰5ä¸ªæ¨¡å‹
    for idx, row in df_results.head(5).iterrows():
        plt.annotate(row['model_name'], 
                    (row['inference_time_per_sample'] * 1000, row['accuracy']),
                    xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig('./Evaluation_Results/Performance_Comparison/accuracy_vs_inference_time.png', 
                dpi=300, bbox_inches='tight')
    plt.close()
    
    # 4. å„ç±»åˆ«F1åˆ†æ•°çƒ­åŠ›å›¾
    class_columns = [col for col in df_results.columns if col.endswith('_f1-score')]
    if class_columns:
        class_f1_data = df_results[['model_name'] + class_columns].set_index('model_name')
        class_f1_data.columns = [col.replace('_f1-score', '') for col in class_f1_data.columns]
        
        plt.figure(figsize=(12, 15))
        sns.heatmap(class_f1_data, annot=True, fmt='.3f', cmap='YlOrRd', 
                   cbar_kws={'label': 'F1-Score'})
        plt.title('å„æ¨¡å‹åœ¨ä¸åŒç±»åˆ«ä¸Šçš„F1åˆ†æ•°', fontsize=16, pad=20)
        plt.xlabel('ç–¾ç—…ç±»åˆ«', fontsize=12)
        plt.ylabel('æ¨¡å‹åç§°', fontsize=12)
        plt.tight_layout()
        plt.savefig('./Evaluation_Results/Performance_Comparison/class_f1_heatmap.png', 
                    dpi=300, bbox_inches='tight')
        plt.close()
    
    # 5. ç»¼åˆæ€§èƒ½å¾—åˆ†
    # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆå¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´æƒé‡ï¼‰
    df_results['composite_score'] = (
        0.3 * df_results['accuracy'] + 
        0.2 * df_results['f1_weighted'] + 
        0.2 * df_results['auc_macro'] + 
        0.15 * df_results['precision_weighted'] + 
        0.15 * df_results['recall_weighted']
    )
    
    plt.figure(figsize=(15, 10))
    df_sorted = df_results.sort_values('composite_score', ascending=True)
    
    # åˆ›å»ºé¢œè‰²æ˜ å°„
    colors = ['red' if score < 0.7 else 'orange' if score < 0.8 else 'green' 
              for score in df_sorted['composite_score']]
    
    plt.barh(df_sorted['model_name'], df_sorted['composite_score'], color=colors)
    plt.xlabel('ç»¼åˆæ€§èƒ½å¾—åˆ†', fontsize=12)
    plt.title('æ¨¡å‹ç»¼åˆæ€§èƒ½è¯„åˆ†ï¼ˆåŠ æƒï¼‰', fontsize=16, pad=20)
    plt.axvline(x=0.8, color='k', linestyle='--', alpha=0.3, label='ä¼˜ç§€é˜ˆå€¼')
    plt.axvline(x=0.7, color='k', linestyle=':', alpha=0.3, label='è‰¯å¥½é˜ˆå€¼')
    plt.legend()
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig('./Evaluation_Results/Performance_Comparison/composite_score_comparison.png', 
                dpi=300, bbox_inches='tight')
    plt.close()

# åªå¤„ç†æˆåŠŸè¯„ä¼°çš„ç»“æœ
successful_results = [r for r in all_results if 'error' not in r]
if successful_results:
    df_results = pd.DataFrame(successful_results)
    df_results = df_results.sort_values('accuracy', ascending=False)
    plot_performance_comparison(df_results)
    print("\nğŸ“Š æ€§èƒ½æ¯”è¾ƒå›¾å·²ç”Ÿæˆå®Œæˆ!")

# ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šæ‘˜è¦
def generate_evaluation_summary(df_results, all_results):
    """ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šæ‘˜è¦"""
    summary = []
    summary.append("="*80)
    summary.append("æ¨¡å‹è¯„ä¼°æŠ¥å‘Šæ‘˜è¦")
    summary.append("="*80)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_models = len(model_names)
    successful_models = len(df_results)
    failed_models = len([r for r in all_results if 'error' in r])
    
    summary.append(f"\nğŸ“Š è¯„ä¼°ç»Ÿè®¡:")
    summary.append(f"   - æ€»æ¨¡å‹æ•°: {total_models}")
    summary.append(f"   - æˆåŠŸè¯„ä¼°: {successful_models}")
    summary.append(f"   - å¤±è´¥è¯„ä¼°: {failed_models}")
    
    if len(df_results) > 0:
        # æœ€ä½³æ¨¡å‹
        best_accuracy = df_results.iloc[0]
        summary.append(f"\nğŸ† å‡†ç¡®ç‡æœ€é«˜æ¨¡å‹: {best_accuracy['model_name']}")
        summary.append(f"   - å‡†ç¡®ç‡: {best_accuracy['accuracy']:.4f}")
        summary.append(f"   - F1åˆ†æ•°: {best_accuracy['f1_weighted']:.4f}")
        summary.append(f"   - AUCå€¼: {best_accuracy['auc_macro']:.4f}")
        
        # æœ€å¿«æ¨¡å‹
        fastest = df_results.loc[df_results['inference_time_per_sample'].idxmin()]
        summary.append(f"\nâš¡ æ¨ç†é€Ÿåº¦æœ€å¿«æ¨¡å‹: {fastest['model_name']}")
        summary.append(f"   - æ¨ç†æ—¶é—´: {fastest['inference_time_per_sample']*1000:.2f} ms/æ ·æœ¬")
        summary.append(f"   - å‡†ç¡®ç‡: {fastest['accuracy']:.4f}")
        
        # æœ€å°æ¨¡å‹
        smallest = df_results.loc[df_results['total_params'].idxmin()]
        summary.append(f"\nğŸ“¦ å‚æ•°é‡æœ€å°‘æ¨¡å‹: {smallest['model_name']}")
        summary.append(f"   - å‚æ•°é‡: {smallest['total_params']:,}")
        summary.append(f"   - å‡†ç¡®ç‡: {smallest['accuracy']:.4f}")
        
        # å¹³å‡æ€§èƒ½
        summary.append(f"\nğŸ“Š æ•´ä½“æ€§èƒ½ç»Ÿè®¡:")
        summary.append(f"   - å¹³å‡å‡†ç¡®ç‡: {df_results['accuracy'].mean():.4f} (Â±{df_results['accuracy'].std():.4f})")
        summary.append(f"   - å¹³å‡F1åˆ†æ•°: {df_results['f1_weighted'].mean():.4f} (Â±{df_results['f1_weighted'].std():.4f})")
        summary.append(f"   - å¹³å‡AUCå€¼: {df_results['auc_macro'].mean():.4f} (Â±{df_results['auc_macro'].std():.4f})")
    
    # å¤±è´¥çš„æ¨¡å‹
    failed_results = [r for r in all_results if 'error' in r]
    if failed_results:
        summary.append(f"\nâŒ è¯„ä¼°å¤±è´¥çš„æ¨¡å‹ ({len(failed_results)}ä¸ª):")
        for result in failed_results:
            summary.append(f"   - {result['model_name']}: {result['error'][:100]}...")
    
    # ä¿å­˜æ‘˜è¦
    with open('./Evaluation_Results/evaluation_summary.txt', 'w', encoding='utf-8') as f:
        f.write('\n'.join(summary))
    
    # æ‰“å°æ‘˜è¦
    print('\n'.join(summary))

if all_results:
    successful_results = [r for r in all_results if 'error' not in r]
    if successful_results:
        df_results = pd.DataFrame(successful_results)
        df_results = df_results.sort_values('accuracy', ascending=False)
        generate_evaluation_summary(df_results, all_results)

print("\nâœ… æ‰€æœ‰è¯„ä¼°ä»»åŠ¡å®Œæˆ!")
print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: ./Evaluation_Results/")
print("   - model_evaluation_results.csv: è¯¦ç»†è¯„ä¼°æŒ‡æ ‡")
print("   - ROC_Curves/: ROCæ›²çº¿å›¾")
print("   - Confusion_Matrices/: æ··æ·†çŸ©é˜µå›¾") 
print("   - PR_Curves/: PRæ›²çº¿å›¾")
print("   - Performance_Comparison/: æ€§èƒ½å¯¹æ¯”å›¾")
print("   - evaluation_summary.txt: è¯„ä¼°æŠ¥å‘Šæ‘˜è¦")
